#
# Config for standard training of ANML for continuous learning on Omniglot.
#

# Dataset
dataset: omni
train_size: 20

# Training Configuration
batch_size: 1
num_batches: 20
train_cycles: 1
val_size: 0
remember_size: 64
remember_only: false
inner_lr: 0.1
outer_lr: 0.001

epochs: 30000
seed: 1

# Model Architecture
model: anml
model_args:
    rln_chs: 256
    nm_chs: 112
    pool_rln_output: false

# Optimization Configuration
inner_params: [rln, fc]
outer_params: all  # special keyword for "all parameters"
output_layer: fc

eval_steps: [10000, 25000]
eval:
    classes: 600
    lr:
      - 0.1
      - 0.05
      - 0.01
      - 0.005
      - 0.001
      - 0.0009
      - 0.0008
      - 0.0007
      - 0.0006
      - 0.0005
      - 0.0004
      - 0.0003
      - 0.0002
      - 0.0001
      - 0.00005
      - 0.00001
